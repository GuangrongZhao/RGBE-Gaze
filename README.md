# RGBE-Gaze:Journey Towards High Frequency Remote Gaze Tracking with Event Cameras
## Introduction RGBE-Gaze

#### 1. RGBE_Gaze_dataset

The **RGBE_Gaze_dataset** folder consists of the data generated by FLIR BFS-U3-16S2C RGB camera (RGB Images), Prophesee EVK4 event camera (Event Streams), Gazepoint GP3 HD (Gaze References) and mouse click positions on the screen (Sparse PoG ground truth).

This directory contains 66 subdirectories corresponding to 66 participants i.e., user_1-user_66. For each participant, the six sessions' data is contained in six separate directory i.e., exp1-exp6. Within each session directory, you will find a 'convert2eventspace' folder containing RGB Images, a 'prophesee' folder containing the Event Stream, a 'gazepoint' folder containing Gaze References, and a 'click_gt_cpu.txt' text file recording Mouse Click Positions alongside corresponding windows CPU timestamps.

  ```
 ─ RGBE_Gaze_dataset
├─user_1
│ ├─eye
│ │ ├─exp1
│ │ │ │ ├─convert2eventspace
│ │ │ │ ├─prophesee
│ │ │ │ ├─gazepoint
│ │ │ │ ├─click_gt_cpu.txt
│ │ │ ├─exp2
│ │ │ │ ..........
│ │ │ └─exp6
├─user_2
..........
├─user_66
  ```
--------------------

#### 2. convert2eventspace
**convert2eventspace** offers the RGB Images converted from the raw RGB Images using the homography matrix T_{h} i.e., 'matlab_processed/tform_total.mat', the homography matrix T_{h} is calculated using the code 'matlab_processed/twoimage_homography.m', and the code transforms the 66 participant's raw RGB Images to align the event stream's space is presented in  'matlab_processed/transfer_image_homography.m'. 

In addition, the **convert2eventspace** folder include three txt file.
- **timestamp.txt** in convert2eventspace records the internal hardware timestamp provided by FLIR BFS-U3-16S2C.
- **timestamp_win.txt** records the Windows system time provided by function 'time.time()' in Python, the first two lines represent the moments before and after the FLIR camera acquisition interface is activated.
- **timestamp_cpu.txt** records the CPU timestamp provided by function 'cv2.getTickCount' of opencv, the first two lines also represent the moments before and after the FLIR camera acquisition interface is activated.



#### 3. Data_tobii
**Data_tobii** includes the gaze references provided by Tobii Pro Glasses 3. The detailed description about `gazedata`, `scenevideo`, `imudata` and `eventdata` can be find in: [https://www.tobii.com/products/eye-trackers/wearables/tobii-pro-glasses-3#form](https://www.tobii.com/products/eye-trackers/wearables/tobii-pro-glasses-3#form). The `tobiisend.txt` file records the system time of the computer when TTL signal is send to Tobii Pro Glasses 3, the `tobiittl.txt` records the TTL signal receiving time in the glasses internal clock. 
  ```
  -Data_tobii
  ├─ user1 
  │  ├─tobiisend.txt
  │  ├─tobiittl.txt
  │  ├─session_1_0_1
  │        ├─gazedata
  │        ├─scenevideo
  │        ├─imudata
  │        ├─eventdata
  |  ..........
  ```
  
--------------------

#### 4. Data_davis_pupil_iris_label

**Data_davis_pupil_iris_label** includes labels of pupil and iris region for frames in a continuous period in each session. The labeled period for each session are recorded in `label_statistic.xlsx`,  the statistics in the file provide information about the state of the eyes during those periods, such as blink and moving direction.

We leverage the [VGG Image Annotator](https://www.robots.ox.ac.uk/~vgg/software/via/via_demo.html) tool to label the pupil region and iris region. The labels in the same image are indexed with `1` and `2`, representing to pupil and iris region, respectively.

 ```
  ─Data_davis_pupil_iris_label
  ├─left
  │  ├─session_1_0_1
  │  │  ├─left101user1_labelled.csv
  │  │  ├─left101user2_labelled.csv
  |  |  ├─left101user3_labelled.csv
  |  |  | ........
  │  ├─session_1_0_2
  │  ├─session_2_0_1
  │  └─session_2_0_2
  ├─right
  └─label_statistic.xlsx
  ```

--------------------

To access more information about the data curation process and data characteristics, kindly refer to Section 3 of the corresponding paper.
<br/>


## Running the Benchmark
Four metrics are adopted for the dataset evaluation, namely **IoU and F1 score**, **Pixel error (PE) of frame-based pupil segmentation**, **PE of event-based pupil tracking**, **Difference of direction (DoD) in gaze tracking**. 

* The **IoU and F1 score** are used to evaluate pupil region segmentation task, and we use pytorch framework in Python to train and evaluate our DL-based Pupil Segmentation network.

* The **PE of frame-based pupil segmentation**, **PE of event-based pupil tracking**, **DoD in gaze tracking** implemented through Matlab code.
 
### Download Dataset

You can download the `raw_data` and `processed_data` in **EV_Eye_dataset** from [https://1drv.ms/f/s!Ar4TcaawWPssqmu-0vJ45vYR3OHw](https://1drv.ms/f/s!Ar4TcaawWPssqmu-0vJ45vYR3OHw) to the `/path/to/EV_Eye_dataset` directory and run the following code to unzip them:

```
cd /path/to/EV_Eye_dataset #choose your own path

#upzip. Following command will extract all the datasets.

find . -mindepth 1 -maxdepth 3 -name '*.rar' -execdir unrar x {} \; -execdir mv {} ./ \;
```

Please place the unzipped data in the `/path/to/EV_Eye_dataset` directory and arrange it according to the following path.

```angular2html
  EV_Eye_dataset
  ├─ raw_data 
  │  ├─Data_davis
  │  ├─Data_davis_labelled_with_mask
  │  ├─Data_tobii
  ├─ processed_data 
  │  ├─Data_davis_predict
  │  ├─Frame_event_pupil_track_result
  │  ├─Pixel_error_evaluation
  │  ├─Pre-trained
```

<!-- By default, our code find the dataset in the `./EV_Eye_dataset` directory. So if you use the default settings, you need to download and unzip the dataset into `./EV_Eye_dataset`. -->

### Python
Note: please use Python >= 3.6.0

#### Python Requirements

```
torch>=1.9.0
numpy>=1.21.0
tqdm>=4.61.1
h5py>=3.2.1
torchvision>=0.10.0
argparse>=1.1
```

To install requirements:

```angular2html
pip install -r requirements.txt
```

#### Training

To train the DL-based Pupil Segmentation network models, run this command:

```
python train.py 
```

Optional arguments can be passed :
* `--whicheye`  to select which eye data to use for training, such as "L" or "R".
* `--batch_size ` 

#### Evaluation of **IoU and F1 score**
The following code provides the calculation method of **IoU and F1 score**:

```
python evaluate.py 
```

#### Predict
```angular2html
python predict.py
```
Optional arguments can be passed :
* `--whicheye`  to select which eye data to use for prediction, such as "L" or "R".
* `--predict` the user ID , for example, '1'. 
* `--output` the output directory for the prediction results, default`./EV_Eye_dataset/processed_data/Data_davis_predict`.

#### Pre-trained models

you can find Pre-trained_models in `./EV_Eye_dataset/processed_data/Pre-trained_models`, it contains our DL-based Pupil Segmentation network pre-trained models using the left and right eyes of each of the 48 participants.

### Matlab
#### Install the Requirement

```angular2html
matlab -batch "pkg install -forge io"
matlab -batch "pkg install -forge curvefit"
```

#### Evaluation of PE of frame-based pupil segmentation & PE of event-based pupil tracking
Run the following codes to estimated the Euclidean distance in pixels between the estimated and manually marked groundtruth pupil centers, and the results  will be saved by default in folder `./EV_Eye_dataset/processed_data/Pixel_error_evaluation/frame` and `./EV_Eye_dataset/processed_data/Pixel_error_evaluation/event`, respectively. You can also find the results that we got before in those folders.
```
./matlab_processed/pe_of_frame_based_pupil_track.m
./matlab_processed/pe_of_event_based_pupil_track.m

#plot the results as bar charts
./matlab_processed/plot_bar_frame_pe.m
./matlab_processed/plot_bar_event_pe.m
``` 

#### Evaluation of DoD in gaze tracking
We use the following matlab scripts to obtain **DoD in gaze tracking**.

First, the following code shows how to obtain frame&event-based pupil tracking results for 48 participants. The results will be saved by default in folder `./EV_Eye_dataset/processed_data/Frame_event_pupil_track_result`, and you can also find the results that we got before in that folder.
```
./matlab_processed/frame_event_pupil_track.m
```
A corresponding visual demonstration is as follows,
```
./matlab_processed/frame_event_pupil_track_plot.m
```
Second, the following code shows how to get the frame&event-based pupil tracking estimated results in folder `./EV_Eye_dataset/processed_data/Frame_event_pupil_track_result` correspond to the reference provided by the Tobii Pro Glasses 3 in  `./EV_Eye_dataset/raw_data/Data_tobii`
```
./matlab_processed/frame_event_pupil_track_result_find_tobii_reference.m
```
Third, the following code shows how to estimate the difference between the estimated and reference gaze directions. 
```
./matlab_processed/evaluation_on_gaze_tracking_with_polynomial_regression.m

#visual 
./matlab_processed/plot_gaze_tracking_dod.m
```



[//]: # (## Results)

[//]: # (##### IoUs and F1 scores on frame-based pupil segmentation.)

[//]: # ()
[//]: # (<br/>)

[//]: # (<div style="display:flex;">)

[//]: # (  <img src="pictures/iou_new.png" alt="iou" style="flex:1;">)

[//]: # (  <img src="pictures/dice.png" alt="iou" style="flex:1;">)

[//]: # (</div>)

[//]: # ()
[//]: # ()
[//]: # (<br/>)

[//]: # ()
[//]: # (##### The pixel error of frame-based and event-based pupil tracking.)

[//]: # ()
[//]: # (<br/>)

[//]: # ()
[//]: # ()
[//]: # (![event]&#40;pictures/event_pixel.png&#41;)

[//]: # (![frame]&#40;pictures/frame_pixel.png&#41;)

[//]: # ()
[//]: # (<br/>)

[//]: # ()
[//]: # (##### DoDs of model-based method vs. ours with respect to the gaze references.)

[//]: # ()
[//]: # (<br/>)

[//]: # ()
[//]: # (<img src="pictures/distance.png" style="margin-left: 6px">)


<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />
This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons
Attribution-NonCommercial 4.0 International License</a>.
